[
  {
    "objectID": "ST 558 - Homework 5.html",
    "href": "ST 558 - Homework 5.html",
    "title": "ST 558 - Homework 5",
    "section": "",
    "text": "You use cross-validation when fitting a random forest model to choose the number of nodes and compare against other models using test set prediction error. The purpose is to prune the tree so you don’t over fit the data, decrease variance and hopefully improve prediction.\nThe bagged tree algorithm is:\n\nBagging = Bootstrap Aggregation - a general method\nBootstrapping\n\nre-sample from the data (non-parametric) or a fitted model (parametric)\nfor non-parametric:\n\ntreats sample as population\nre-sampling done with replacement\ncan get some observation multiple times\n\nmethod or estimation applied to each re-sample\ntraditionally used to obtain standard errors (measure of variability) or consturct confidence intervals\n\n\nA general linear model:\n\nContinuous response\nAllows for both continuous and categorical predictors\n\nWhen fitting a multiple linear regression model, adding an interaction term allows the model to capture the combined effect of two independent variables on the dependent variable, that would otherwise not be accounted for in the model.\nWe split our data into a training and test set so that we don’t test our model on the same data we trained it on. We don’t want the model to be trained to much to our data set our will only be good at predicting for that data set and not for new data."
  },
  {
    "objectID": "ST 558 - Homework 5.html#task-1-conceptual-questions",
    "href": "ST 558 - Homework 5.html#task-1-conceptual-questions",
    "title": "ST 558 - Homework 5",
    "section": "",
    "text": "You use cross-validation when fitting a random forest model to choose the number of nodes and compare against other models using test set prediction error. The purpose is to prune the tree so you don’t over fit the data, decrease variance and hopefully improve prediction.\nThe bagged tree algorithm is:\n\nBagging = Bootstrap Aggregation - a general method\nBootstrapping\n\nre-sample from the data (non-parametric) or a fitted model (parametric)\nfor non-parametric:\n\ntreats sample as population\nre-sampling done with replacement\ncan get some observation multiple times\n\nmethod or estimation applied to each re-sample\ntraditionally used to obtain standard errors (measure of variability) or consturct confidence intervals\n\n\nA general linear model:\n\nContinuous response\nAllows for both continuous and categorical predictors\n\nWhen fitting a multiple linear regression model, adding an interaction term allows the model to capture the combined effect of two independent variables on the dependent variable, that would otherwise not be accounted for in the model.\nWe split our data into a training and test set so that we don’t test our model on the same data we trained it on. We don’t want the model to be trained to much to our data set our will only be good at predicting for that data set and not for new data."
  },
  {
    "objectID": "ST 558 - Homework 5.html#task-2-fitting-models",
    "href": "ST 558 - Homework 5.html#task-2-fitting-models",
    "title": "ST 558 - Homework 5",
    "section": "Task 2: Fitting Models",
    "text": "Task 2: Fitting Models\n\n# All libraries needed\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(gbm)\n\n\nheart_data &lt;- read_csv(\"heart.csv\")\n\nRows: 918 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Sex, ChestPainType, RestingECG, ExerciseAngina, ST_Slope\ndbl (7): Age, RestingBP, Cholesterol, FastingBS, MaxHR, Oldpeak, HeartDisease\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(heart_data)\n\n# A tibble: 6 × 12\n    Age Sex   ChestPainType RestingBP Cholesterol FastingBS RestingECG MaxHR\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;             &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n1    40 M     ATA                 140         289         0 Normal       172\n2    49 F     NAP                 160         180         0 Normal       156\n3    37 M     ATA                 130         283         0 ST            98\n4    48 F     ASY                 138         214         0 Normal       108\n5    54 M     NAP                 150         195         0 Normal       122\n6    39 M     NAP                 120         339         0 Normal       170\n# ℹ 4 more variables: ExerciseAngina &lt;chr&gt;, Oldpeak &lt;dbl&gt;, ST_Slope &lt;chr&gt;,\n#   HeartDisease &lt;dbl&gt;\n\n\n\nQuick EDA/Data Preparation\n\nPart 1\n\nQuickly understand your data. Check on missingness and summarize the data, especially with respect to the relationships of the variables to HeartDisease.\n\n\nStructure of heart data set:\n\n\nstr(heart_data)\n\nspc_tbl_ [918 × 12] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Age           : num [1:918] 40 49 37 48 54 39 45 54 37 48 ...\n $ Sex           : chr [1:918] \"M\" \"F\" \"M\" \"F\" ...\n $ ChestPainType : chr [1:918] \"ATA\" \"NAP\" \"ATA\" \"ASY\" ...\n $ RestingBP     : num [1:918] 140 160 130 138 150 120 130 110 140 120 ...\n $ Cholesterol   : num [1:918] 289 180 283 214 195 339 237 208 207 284 ...\n $ FastingBS     : num [1:918] 0 0 0 0 0 0 0 0 0 0 ...\n $ RestingECG    : chr [1:918] \"Normal\" \"Normal\" \"ST\" \"Normal\" ...\n $ MaxHR         : num [1:918] 172 156 98 108 122 170 170 142 130 120 ...\n $ ExerciseAngina: chr [1:918] \"N\" \"N\" \"N\" \"Y\" ...\n $ Oldpeak       : num [1:918] 0 1 0 1.5 0 0 0 0 1.5 0 ...\n $ ST_Slope      : chr [1:918] \"Up\" \"Flat\" \"Up\" \"Flat\" ...\n $ HeartDisease  : num [1:918] 0 1 0 1 0 0 0 0 1 0 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Age = col_double(),\n  ..   Sex = col_character(),\n  ..   ChestPainType = col_character(),\n  ..   RestingBP = col_double(),\n  ..   Cholesterol = col_double(),\n  ..   FastingBS = col_double(),\n  ..   RestingECG = col_character(),\n  ..   MaxHR = col_double(),\n  ..   ExerciseAngina = col_character(),\n  ..   Oldpeak = col_double(),\n  ..   ST_Slope = col_character(),\n  ..   HeartDisease = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\n\nSummary of heart data set:\n\n\nsummary(heart_data)\n\n      Age            Sex            ChestPainType        RestingBP    \n Min.   :28.00   Length:918         Length:918         Min.   :  0.0  \n 1st Qu.:47.00   Class :character   Class :character   1st Qu.:120.0  \n Median :54.00   Mode  :character   Mode  :character   Median :130.0  \n Mean   :53.51                                         Mean   :132.4  \n 3rd Qu.:60.00                                         3rd Qu.:140.0  \n Max.   :77.00                                         Max.   :200.0  \n  Cholesterol      FastingBS       RestingECG            MaxHR      \n Min.   :  0.0   Min.   :0.0000   Length:918         Min.   : 60.0  \n 1st Qu.:173.2   1st Qu.:0.0000   Class :character   1st Qu.:120.0  \n Median :223.0   Median :0.0000   Mode  :character   Median :138.0  \n Mean   :198.8   Mean   :0.2331                      Mean   :136.8  \n 3rd Qu.:267.0   3rd Qu.:0.0000                      3rd Qu.:156.0  \n Max.   :603.0   Max.   :1.0000                      Max.   :202.0  \n ExerciseAngina        Oldpeak          ST_Slope          HeartDisease   \n Length:918         Min.   :-2.6000   Length:918         Min.   :0.0000  \n Class :character   1st Qu.: 0.0000   Class :character   1st Qu.:0.0000  \n Mode  :character   Median : 0.6000   Mode  :character   Median :1.0000  \n                    Mean   : 0.8874                      Mean   :0.5534  \n                    3rd Qu.: 1.5000                      3rd Qu.:1.0000  \n                    Max.   : 6.2000                      Max.   :1.0000  \n\n\n\nRate of missing values:\n\n\n# Create a function to count number of missing values\nsum_na &lt;- function(column){\n sum(is.na(column))\n}\n\n# Use above function across all variables in heart data set\nna_counts &lt;- heart_data |&gt;\n summarize(across(everything(), sum_na))\nna_counts\n\n# A tibble: 1 × 12\n    Age   Sex ChestPainType RestingBP Cholesterol FastingBS RestingECG MaxHR\n  &lt;int&gt; &lt;int&gt;         &lt;int&gt;     &lt;int&gt;       &lt;int&gt;     &lt;int&gt;      &lt;int&gt; &lt;int&gt;\n1     0     0             0         0           0         0          0     0\n# ℹ 4 more variables: ExerciseAngina &lt;int&gt;, Oldpeak &lt;int&gt;, ST_Slope &lt;int&gt;,\n#   HeartDisease &lt;int&gt;\n\n\n\nRelationship between variables:\n\n\n# Select only numeric variables for correlation\nnumeric_vars &lt;- heart_data |&gt; \n  select_if(is.numeric)\n\n# Calculate correlation matrix\ncor(numeric_vars)\n\n                     Age   RestingBP Cholesterol   FastingBS      MaxHR\nAge           1.00000000  0.25439936 -0.09528177  0.19803907 -0.3820447\nRestingBP     0.25439936  1.00000000  0.10089294  0.07019334 -0.1121350\nCholesterol  -0.09528177  0.10089294  1.00000000 -0.26097433  0.2357924\nFastingBS     0.19803907  0.07019334 -0.26097433  1.00000000 -0.1314385\nMaxHR        -0.38204468 -0.11213500  0.23579240 -0.13143849  1.0000000\nOldpeak       0.25861154  0.16480304  0.05014811  0.05269786 -0.1606906\nHeartDisease  0.28203851  0.10758898 -0.23274064  0.26729119 -0.4004208\n                 Oldpeak HeartDisease\nAge           0.25861154    0.2820385\nRestingBP     0.16480304    0.1075890\nCholesterol   0.05014811   -0.2327406\nFastingBS     0.05269786    0.2672912\nMaxHR        -0.16069055   -0.4004208\nOldpeak       1.00000000    0.4039507\nHeartDisease  0.40395072    1.0000000\n\n\n\n\nPart 2\n\nCreate a new variable that is a factor version of the HeartDisease variable\n\n\nheart_data$HeartDisease &lt;- as.factor(heart_data$HeartDisease)\n\n\nRemove the ST_Slope variable\n\n\nheart_data &lt;- heart_data |&gt;\n  select(-ST_Slope)\n\nhead(heart_data)\n\n# A tibble: 6 × 11\n    Age Sex   ChestPainType RestingBP Cholesterol FastingBS RestingECG MaxHR\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;             &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n1    40 M     ATA                 140         289         0 Normal       172\n2    49 F     NAP                 160         180         0 Normal       156\n3    37 M     ATA                 130         283         0 ST            98\n4    48 F     ASY                 138         214         0 Normal       108\n5    54 M     NAP                 150         195         0 Normal       122\n6    39 M     NAP                 120         339         0 Normal       170\n# ℹ 3 more variables: ExerciseAngina &lt;chr&gt;, Oldpeak &lt;dbl&gt;, HeartDisease &lt;fct&gt;\n\n\n\n\nPart 3\n\nCreate dummy columns corresponding to the values of the variables Sex, ExerciseAngina, ChestPainType, and RestingECG for use in our kNN fit. The caret vignette has a function to help us out here. You should use dummyVars() and predict() to create new columns. Then add these columns to our data frame.\n\n\n# Use dummyVars and predict to create new columns\ndummy_vars &lt;- dummyVars(~ Sex + ExerciseAngina + ChestPainType + RestingECG, data = heart_data)\ndummy_data &lt;- predict(dummy_vars, newdata = heart_data)\nheart_data &lt;- cbind(heart_data, dummy_data) \n\nhead(heart_data)\n\n  Age Sex ChestPainType RestingBP Cholesterol FastingBS RestingECG MaxHR\n1  40   M           ATA       140         289         0     Normal   172\n2  49   F           NAP       160         180         0     Normal   156\n3  37   M           ATA       130         283         0         ST    98\n4  48   F           ASY       138         214         0     Normal   108\n5  54   M           NAP       150         195         0     Normal   122\n6  39   M           NAP       120         339         0     Normal   170\n  ExerciseAngina Oldpeak HeartDisease SexF SexM ExerciseAnginaN ExerciseAnginaY\n1              N     0.0            0    0    1               1               0\n2              N     1.0            1    1    0               1               0\n3              N     0.0            0    0    1               1               0\n4              Y     1.5            1    1    0               0               1\n5              N     0.0            0    0    1               1               0\n6              N     0.0            0    0    1               1               0\n  ChestPainTypeASY ChestPainTypeATA ChestPainTypeNAP ChestPainTypeTA\n1                0                1                0               0\n2                0                0                1               0\n3                0                1                0               0\n4                1                0                0               0\n5                0                0                1               0\n6                0                0                1               0\n  RestingECGLVH RestingECGNormal RestingECGST\n1             0                1            0\n2             0                1            0\n3             0                0            1\n4             0                1            0\n5             0                1            0\n6             0                1            0\n\n\n\n\n\nSplit your Data\n\nSplit your data into a training and test set.\n\n\n# Use caret package to split data into training and test sets\ntrainIndex &lt;- createDataPartition(heart_data$HeartDisease, p = 0.1, list = FALSE)\nheartTrain &lt;- heart_data[trainIndex, ]\nheartTest &lt;- heart_data[-trainIndex, ]\n\n\n\nkNN\n\nNext, we’ll fit a kNN model.\n\n\nTrain the kNN model. Use repeated 10 fold cross-validation, with the number of repeats being 3. You should also preprocess the data by centering and scaling. When fitting the model, set the tuneGrid so that you are considering values of k of 1, 2, 3, . . . , 40.\n\n\n# Train the kNN model\nknn_model &lt;- train(HeartDisease ~ SexF + SexM + Age + Cholesterol + RestingBP,\n                   data = heartTrain,\n                   method = \"knn\",\n                   trControl = trainControl(method = \"repeatedcv\", number = 10, \n                                            repeats = 3, preProc = c(\"center\", \"scale\")),\n                   tuneGrid = expand.grid(k = 1:40))\n\nknn_model\n\nk-Nearest Neighbors \n\n92 samples\n 5 predictor\n 2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 83, 83, 83, 83, 83, 82, ... \nResampling results across tuning parameters:\n\n  k   Accuracy   Kappa    \n   1  0.6303704  0.2581615\n   2  0.6229630  0.2333288\n   3  0.6692593  0.3307182\n   4  0.6329630  0.2509263\n   5  0.6762963  0.3385949\n   6  0.6866667  0.3576920\n   7  0.7188889  0.4206520\n   8  0.7159259  0.4090132\n   9  0.7051852  0.3915309\n  10  0.6837037  0.3477268\n  11  0.6977778  0.3756914\n  12  0.6544444  0.2901288\n  13  0.6548148  0.2967375\n  14  0.6333333  0.2476511\n  15  0.5977778  0.1851472\n  16  0.6144444  0.2243038\n  17  0.6140741  0.2214411\n  18  0.6218519  0.2351864\n  19  0.6107407  0.2178507\n  20  0.6103704  0.2123771\n  21  0.5892593  0.1680027\n  22  0.5896296  0.1667698\n  23  0.6033333  0.2003274\n  24  0.6040741  0.2055230\n  25  0.5633333  0.1298845\n  26  0.5751852  0.1472175\n  27  0.5603704  0.1155793\n  28  0.5788889  0.1527633\n  29  0.5707407  0.1293604\n  30  0.5596296  0.1081423\n  31  0.5681481  0.1282467\n  32  0.5685185  0.1295657\n  33  0.5862963  0.1606816\n  34  0.5859259  0.1553303\n  35  0.5825926  0.1508308\n  36  0.5862963  0.1643902\n  37  0.5822222  0.1554952\n  38  0.5937037  0.1801433\n  39  0.5962963  0.1870358\n  40  0.6000000  0.2003637\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was k = 7.\n\n\n\nLastly, check how well your chosen model does on the test set using the confusionMatrix() function.\n\n\nknn_preds &lt;- predict(knn_model, newdata = heartTest)\nconfusionMatrix(knn_preds, heartTest$HeartDisease)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 175 123\n         1 194 334\n                                          \n               Accuracy : 0.6162          \n                 95% CI : (0.5821, 0.6495)\n    No Information Rate : 0.5533          \n    P-Value [Acc &gt; NIR] : 0.0001451       \n                                          \n                  Kappa : 0.209           \n                                          \n Mcnemar's Test P-Value : 8.439e-05       \n                                          \n            Sensitivity : 0.4743          \n            Specificity : 0.7309          \n         Pos Pred Value : 0.5872          \n         Neg Pred Value : 0.6326          \n             Prevalence : 0.4467          \n         Detection Rate : 0.2119          \n   Detection Prevalence : 0.3608          \n      Balanced Accuracy : 0.6026          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\n\nLogistic Regression\n\nUsing your EDA, posit three different logistic regression models.\n\n\nFit those models on the training set, using repeated CV as done above. You can preprocess the data or not, up to you.\n\n\n# First Logistic Regression model\nlog_reg1 &lt;- train(HeartDisease ~ SexF + SexM + Age + Cholesterol + RestingBP , \n                  data = heartTrain,\n                  method = \"glmnet\",\n                  family = \"binomial\",\n                  trControl = trainControl(method = \"repeatedcv\", number = 10, \n                                            repeats = 3, preProc = c(\"center\", \"scale\")))\n\n# Second Logistic Regression model\nlog_reg2 &lt;- train(HeartDisease ~ SexF + SexM + Age:Cholesterol + RestingBP, \n                  data = heartTrain,\n                  method = \"glmnet\",\n                  family = \"binomial\",\n                  trControl = trainControl(method = \"repeatedcv\", number = 10, \n                                            repeats = 3, preProc = c(\"center\", \"scale\")))\n\n# Third Logistic Regression model\nlog_reg3 &lt;- train(HeartDisease ~ SexF + SexM + Age + Cholesterol + RestingBP^2, \n                  data = heartTrain,\n                  method = \"glmnet\",\n                  family = \"binomial\",\n                  trControl = trainControl(method = \"repeatedcv\", number = 10, \n                                            repeats = 3, preProc = c(\"center\", \"scale\")))\n\n\nIdentify your best model and provide a basic summary of it.\n\n\nlog_reg1_preds &lt;- predict(log_reg1, newdata = heartTest)\nlog_reg2_preds &lt;- predict(log_reg2, newdata = heartTest)\nlog_reg3_preds &lt;- predict(log_reg3, newdata = heartTest)\n\npostResample(log_reg1_preds, heartTest$HeartDisease)\n\n Accuracy     Kappa \n0.6815981 0.3430289 \n\npostResample(log_reg2_preds, heartTest$HeartDisease)\n\n Accuracy     Kappa \n0.6307506 0.2278386 \n\npostResample(log_reg3_preds, heartTest$HeartDisease)\n\n Accuracy     Kappa \n0.6815981 0.3430289 \n\n\n\nBetween these three logistic regression models, the first and third model did the best. For simplicity we will use the first\n\n\nprint(log_reg1)\n\nglmnet \n\n92 samples\n 5 predictor\n 2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 82, 83, 83, 83, 82, 83, ... \nResampling results across tuning parameters:\n\n  alpha  lambda       Accuracy   Kappa    \n  0.10   0.000356585  0.6744444  0.3330613\n  0.10   0.003565850  0.6744444  0.3330613\n  0.10   0.035658501  0.6925926  0.3684692\n  0.55   0.000356585  0.6744444  0.3330613\n  0.55   0.003565850  0.6744444  0.3330613\n  0.55   0.035658501  0.6814815  0.3449453\n  1.00   0.000356585  0.6670370  0.3167920\n  1.00   0.003565850  0.6670370  0.3167920\n  1.00   0.035658501  0.6740741  0.3287914\n\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were alpha = 0.1 and lambda = 0.0356585.\n\n\n\nLastly, check how well your chosen model does on the test set using the confusionMatrix() function.\n\n\nlog_reg_preds &lt;- predict(log_reg1, newdata = heartTest)\nconfusionMatrix(log_reg_preds, heartTest$HeartDisease)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 200  94\n         1 169 363\n                                          \n               Accuracy : 0.6816          \n                 95% CI : (0.6486, 0.7133)\n    No Information Rate : 0.5533          \n    P-Value [Acc &gt; NIR] : 3.312e-14       \n                                          \n                  Kappa : 0.343           \n                                          \n Mcnemar's Test P-Value : 5.042e-06       \n                                          \n            Sensitivity : 0.5420          \n            Specificity : 0.7943          \n         Pos Pred Value : 0.6803          \n         Neg Pred Value : 0.6823          \n             Prevalence : 0.4467          \n         Detection Rate : 0.2421          \n   Detection Prevalence : 0.3559          \n      Balanced Accuracy : 0.6682          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\n\nTree Models\n\nChoose your own variables of interest (as with logistic regression, this models can accept factor/character variables as predictors). Use repated 10 fold CV to select a best\n\n\n\nclassification tree model (use method = rpart: tuning parameter is cp, use values 0, 0.001, 0.002,…, 0.1)\na random forest (use method = rf: tuning parameter is mtry, use values of 1, 2, . . . , # of predictors (bagging is a special case here!)\na boosted tree (use method = gbm: tuning parameters are n.trees, interaction.depth, shrinkage, and n.minobsinnode, use all combinations of n.trees of 25, 50, 100, and 200, interaction.depth of 1, 2, 3, shrinkage = 0.1, and nminobsinnode = 10; Hint: use expand.grid() to create your data frame for tuneGrid and verbose = FALSE limits the output produced\n\n\n\n# Train  classification tree\nclass_tree &lt;- train(HeartDisease ~ SexF + SexM + Age + Cholesterol + RestingBP,\n                    data = heartTrain,\n                    method = \"rpart\",\n                    trControl = trainControl(method = \"repeatedcv\", number = 10, repeats = 3),\n                    tuneGrid = expand.grid(cp = seq(0, 0.1, by = 0.001)))\n\n# Train random forest\nrand_for &lt;- train(HeartDisease ~ SexF + SexM + Age + Cholesterol + RestingBP,\n                  data = heartTrain,\n                  method = \"rf\",\n                  trControl = trainControl(method = \"repeatedcv\", number = 10, repeats = 3),\n                  tuneGrid = expand.grid(mtry = 1:5))\n\n# Train boosted tree\nboost_tree &lt;- train(HeartDisease ~ .,\n                  data = heartTrain,\n                  method = \"gbm\",\n                  trControl = trainControl(method = \"repeatedcv\", number = 10, repeats = 3),\n                  tuneGrid = expand.grid(n.trees = c(25, 50, 100, 200),\n                                         interaction.depth = 1:3,\n                                         shrinkage = 0.1,\n                                         n.minobsinnode = 10),\n                  verbose = FALSE)\n\n\nLastly, check how well each of your chosen models do on the test set using the confusionMatrix() function.\n\n\nclass_tree_preds &lt;- predict(class_tree, newdata = heartTest)\nrand_for_preds &lt;- predict(rand_for, newdata = heartTest)\nboost_tree_preds &lt;- predict(boost_tree, newdata = heartTest)\n\n\nClassification Tree\n\n\nconfusionMatrix(class_tree_preds, heartTest$HeartDisease)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 192 117\n         1 177 340\n                                          \n               Accuracy : 0.6441          \n                 95% CI : (0.6103, 0.6768)\n    No Information Rate : 0.5533          \n    P-Value [Acc &gt; NIR] : 7.137e-08       \n                                          \n                  Kappa : 0.2685          \n                                          \n Mcnemar's Test P-Value : 0.0005797       \n                                          \n            Sensitivity : 0.5203          \n            Specificity : 0.7440          \n         Pos Pred Value : 0.6214          \n         Neg Pred Value : 0.6576          \n             Prevalence : 0.4467          \n         Detection Rate : 0.2324          \n   Detection Prevalence : 0.3741          \n      Balanced Accuracy : 0.6322          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\nRandom Forest\n\n\nconfusionMatrix(rand_for_preds, heartTest$HeartDisease)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 187 104\n         1 182 353\n                                          \n               Accuracy : 0.6538          \n                 95% CI : (0.6202, 0.6862)\n    No Information Rate : 0.5533          \n    P-Value [Acc &gt; NIR] : 2.691e-09       \n                                          \n                  Kappa : 0.285           \n                                          \n Mcnemar's Test P-Value : 5.286e-06       \n                                          \n            Sensitivity : 0.5068          \n            Specificity : 0.7724          \n         Pos Pred Value : 0.6426          \n         Neg Pred Value : 0.6598          \n             Prevalence : 0.4467          \n         Detection Rate : 0.2264          \n   Detection Prevalence : 0.3523          \n      Balanced Accuracy : 0.6396          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\nBoosted Tree\n\n\nconfusionMatrix(boost_tree_preds, heartTest$HeartDisease)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 294  81\n         1  75 376\n                                          \n               Accuracy : 0.8111          \n                 95% CI : (0.7827, 0.8373)\n    No Information Rate : 0.5533          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.6185          \n                                          \n Mcnemar's Test P-Value : 0.6889          \n                                          \n            Sensitivity : 0.7967          \n            Specificity : 0.8228          \n         Pos Pred Value : 0.7840          \n         Neg Pred Value : 0.8337          \n             Prevalence : 0.4467          \n         Detection Rate : 0.3559          \n   Detection Prevalence : 0.4540          \n      Balanced Accuracy : 0.8098          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\n\nWrap up\n\nIn terms of accuracy, the Boosted Tree performed the best.\n\n\npostResample(boost_tree_preds, heartTest$HeartDisease)\n\n Accuracy     Kappa \n0.8111380 0.6185367"
  }
]